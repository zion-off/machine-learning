{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c422e1b",
   "metadata": {},
   "source": [
    "### Import libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f3ba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "## Avoid printing out warnings\n",
    "with warnings.catch_warnings():\n",
    "     warnings.filterwarnings(\"ignore\")\n",
    "     X, y = load_boston(return_X_y=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368a8947",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "print(boston_dataset.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8f8345",
   "metadata": {},
   "source": [
    "### Closed form solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff19f9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# normalize dataset\n",
    "u = np.mean(X, axis=0)\n",
    "std = np.std(X, axis=0)\n",
    "X = (X - u)/std\n",
    "\n",
    "# append the column of 1s\n",
    "one = np.ones((X.shape[0], 1))\n",
    "X = np.hstack((one, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f05fe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of folds for cross-validation\n",
    "k = 10\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "mse_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ba1e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform k-fold cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    # (X.TX)-1 X.Ty\n",
    "    a = np.linalg.inv(np.dot(X_train.T, X_train))\n",
    "    b = np.dot(X_train.T, y_train)\n",
    "    theta = np.dot(a, b)\n",
    "\n",
    "    y_prediction = np.dot(X_test, theta)\n",
    "    \n",
    "    mse_fold = np.mean((y_test - y_prediction) ** 2)\n",
    "    mse_values.append(mse_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c825f826",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_mse = np.mean(mse_values)\n",
    "print(\"Average MSE across\", k, \"folds:\", average_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cbb4d7",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2a7e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# normalize dataset\n",
    "u = np.mean(X, axis=0)\n",
    "std = np.std(X, axis=0)\n",
    "X = (X - u)/std\n",
    "\n",
    "# append the column of 1s\n",
    "one = np.ones((X.shape[0], 1))\n",
    "X = np.hstack((one, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5862187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of folds for cross-validation\n",
    "k = 10\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# define the range of lambda values using np.logspace\n",
    "lambda_values = np.logspace(1, 7, num=13)\n",
    "\n",
    "# initialize a dictionary to store the mse scores for each lambda value\n",
    "mse_values_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe6f09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform k-fold cross-validation for each lambda value\n",
    "for lambda_param in lambda_values:\n",
    "    mse_values = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # (X.TX+λI)-1.X.Ty\n",
    "        XTX = np.dot(X_train.T, X_train)\n",
    "        I = np.eye(XTX.shape[0])\n",
    "        theta = np.dot(np.linalg.inv(XTX + lambda_param * I), np.dot(X_train.T, y_train))\n",
    "\n",
    "        # calculate MSE for this fold\n",
    "        y_prediction = np.dot(X_test, theta)\n",
    "        mse_fold = np.mean((y_test - y_prediction) ** 2)\n",
    "        mse_values.append(mse_fold)\n",
    "\n",
    "    # store the MSE values for this lambda value\n",
    "    mse_values_dict[lambda_param] = np.mean(mse_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e299e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the lambda value with the lowest average mse\n",
    "best_lambda = min(mse_values_dict, key=mse_values_dict.get)\n",
    "best_mse = mse_values_dict[best_lambda]\n",
    "\n",
    "print(f'Best lambda: 10^{int(np.log10(best_lambda))}')\n",
    "print(f'Corresponding average MSE: {best_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b36ac9",
   "metadata": {},
   "source": [
    "### Evaluating model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf98d6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# normalize dataset\n",
    "u = np.mean(X, axis=0)\n",
    "std = np.std(X, axis=0)\n",
    "X = (X - u)/std\n",
    "\n",
    "# append the column of 1s\n",
    "one = np.ones((X.shape[0], 1))\n",
    "X = np.hstack((one, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36abb077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of folds for cross-validation\n",
    "k = 10\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# define the range of lambda values using np.logspace\n",
    "lambda_values = np.logspace(1, 7, num=13)\n",
    "\n",
    "# initialize a list to store the MSE scores for best lambda value\n",
    "mse_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b42d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform k-fold cross-validation for best lambda value\n",
    "for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # (X.TX+λI)-1.X.Ty\n",
    "        XTX = np.dot(X_train.T, X_train)\n",
    "        I = np.eye(XTX.shape[0])\n",
    "        theta = np.dot(np.linalg.inv(XTX + best_lambda * I), np.dot(X_train.T, y_train))\n",
    "\n",
    "        y_prediction_train = np.dot(X_train, theta)\n",
    "        y_prediction_test = np.dot(X_test, theta)\n",
    "\n",
    "        mse_train = np.mean((y_train - y_prediction_train) ** 2)\n",
    "        mse_test = np.mean((y_test - y_prediction_test) ** 2)\n",
    "\n",
    "        mse_scores.append({'train': mse_train, 'test': mse_test})\n",
    "\n",
    "average_mse_train = np.mean([fold['train'] for fold in mse_scores])\n",
    "average_mse_test = np.mean([fold['test'] for fold in mse_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7413db20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'average mse on training set: {average_mse_train}')\n",
    "print(f'average mse on test set: {average_mse_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6e17c0",
   "metadata": {},
   "source": [
    "### Polynomial transformation and ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef8951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# normalize dataset\n",
    "u = np.mean(X, axis=0)\n",
    "std = np.std(X, axis=0)\n",
    "X = (X - u) / std\n",
    "\n",
    "# create polynomial features of degree 2\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# append the column of 1s\n",
    "one = np.ones((X_poly.shape[0], 1))\n",
    "X_poly = np.hstack((one, X_poly))\n",
    "\n",
    "# define the number of folds (e.g., 5)\n",
    "k = 10\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# define the range of lambda values using np.logspace\n",
    "lambda_values = np.logspace(1, 7, num=13)\n",
    "\n",
    "# initialize a dictionary to store the mse scores for each lambda value\n",
    "mse_scores_dict = {}\n",
    "\n",
    "# perform k-fold cross-validation for each lambda value\n",
    "for lambda_param in lambda_values:\n",
    "    mse_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X_poly):\n",
    "        X_train, X_test = X_poly[train_index], X_poly[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        XTX = np.dot(X_train.T, X_train)\n",
    "        I = np.eye(XTX.shape[0])\n",
    "        theta = np.dot(np.linalg.inv(XTX + lambda_param * I), np.dot(X_train.T, y_train))\n",
    "\n",
    "        y_prediction_train = np.dot(X_train, theta)\n",
    "        y_prediction_test = np.dot(X_test, theta)\n",
    "\n",
    "        mse_train = np.mean((y_train - y_prediction_train) ** 2)\n",
    "        mse_test = np.mean((y_test - y_prediction_test) ** 2)\n",
    "\n",
    "        mse_scores.append({'train': mse_train, 'test': mse_test})\n",
    "\n",
    "    # store the average mse scores for this lambda value\n",
    "    mse_scores_dict[lambda_param] = {\n",
    "        'train': np.mean([fold['train'] for fold in mse_scores]),\n",
    "        'test': np.mean([fold['test'] for fold in mse_scores])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f594775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the lambda value with the lowest average mse on the test set\n",
    "best_lambda = min(mse_scores_dict, key=lambda k: mse_scores_dict[k]['test'])\n",
    "best_mse_train = mse_scores_dict[best_lambda]['train']\n",
    "best_mse_test = mse_scores_dict[best_lambda]['test']\n",
    "\n",
    "print(f'best lambda: 10^{int(np.log10(best_lambda))}')\n",
    "print(f'average mse on training set: {best_mse_train}')\n",
    "print(f'average mse on test set: {best_mse_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab24ade",
   "metadata": {},
   "source": [
    "### Multivariate Linear Regression using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a35722",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# normalize dataset\n",
    "u = np.mean(X, axis=0)\n",
    "std = np.std(X, axis=0)\n",
    "X = (X - u) / std\n",
    "\n",
    "# create polynomial features of degree 2\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# append the column of 1s\n",
    "one = np.ones((X_poly.shape[0], 1))\n",
    "X_poly = np.hstack((one, X_poly))\n",
    "\n",
    "# initialize parameters\n",
    "theta = np.zeros(X_poly.shape[1])\n",
    "\n",
    "# hyperparameters\n",
    "L = 0.01  # learning rate\n",
    "epochs = 1000\n",
    "\n",
    "# define the number of folds\n",
    "k = 10\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# initialize lists to store mse values for each fold\n",
    "mse_train_list = []\n",
    "mse_test_list = []\n",
    "\n",
    "# perform k-fold cross-validation\n",
    "for train_index, test_index in kf.split(X_poly):\n",
    "    X_train, X_test = X_poly[train_index], X_poly[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # gradient descent\n",
    "    for iteration in range(epochs):\n",
    "        # calculate predictions\n",
    "        predictions_train = np.dot(X_train, theta)\n",
    "        predictions_test = np.dot(X_test, theta)\n",
    "\n",
    "        # calculate MSE for training set\n",
    "        mse_train = np.mean((predictions_train - y_train) ** 2)\n",
    "\n",
    "        # calculate MSE for test set\n",
    "        mse_test = np.mean((predictions_test - y_test) ** 2)\n",
    "\n",
    "        # update parameters using gradient\n",
    "        gradient = np.dot(X_train.T, predictions_train - y_train) / len(y_train)\n",
    "        theta -= L * gradient\n",
    "\n",
    "    # append mse values to the lists\n",
    "    mse_train_list.append(mse_train)\n",
    "    mse_test_list.append(mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb8156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate and print the average MSE values\n",
    "average_mse_train = np.mean(mse_train_list)\n",
    "average_mse_test = np.mean(mse_test_list)\n",
    "\n",
    "print(f'average mse on training set: {average_mse_train}')\n",
    "print(f'average mse on test set: {average_mse_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773b6389",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5e71ff06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average mean squared error across 10 folds: 23.340154343671713\n"
     ]
    }
   ],
   "source": [
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# normalize features\n",
    "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "# add a column of ones for the intercept term\n",
    "X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "# initialize parameters\n",
    "theta = np.zeros(X.shape[1])\n",
    "\n",
    "# hyperparameters\n",
    "alpha = 0.01  # learning rate\n",
    "lambda_param = 0.1  # regularization parameter\n",
    "num_iterations = 1000\n",
    "\n",
    "# define the number of folds\n",
    "k = 10\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# initialize lists to store MSE values for each fold\n",
    "mse_list = []\n",
    "\n",
    "# perform k-fold cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # gradient Descent\n",
    "    for iteration in range(num_iterations):\n",
    "        # calculate predictions\n",
    "        predictions_train = np.dot(X_train, theta)\n",
    "\n",
    "        # calculate errors\n",
    "        errors = predictions_train - y_train\n",
    "\n",
    "        # update parameters using gradient with L1 penalty\n",
    "        l1_penalty = (lambda_param / len(y_train)) * np.sign(theta)\n",
    "        l1_penalty[0] = 0\n",
    "        gradient = (1 / len(y_train)) * np.dot(X_train.T, errors) + l1_penalty\n",
    "        theta -= alpha * gradient\n",
    "\n",
    "    # evaluate on the test set for this fold\n",
    "    predictions_test = np.dot(X_test, theta)\n",
    "    mse_test = np.mean((predictions_test - y_test) ** 2)\n",
    "\n",
    "    mse_list.append(mse_test)\n",
    "\n",
    "# calculate and print the average MSE value across all folds\n",
    "average_mse = np.mean(mse_list)\n",
    "print(f'average mean squared error across {k} folds: {average_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074787ee",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d6ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# normalize features\n",
    "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "# add a column of ones for the intercept term\n",
    "X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "# initialize parameters\n",
    "theta = np.zeros(X.shape[1])\n",
    "\n",
    "# hyperparameters\n",
    "alpha = 0.01  # learning rate\n",
    "lambda_1 = 0.1  # L1 regularization parameter\n",
    "lambda_2 = 0.1  # L2 regularization parameter\n",
    "num_iterations = 1000\n",
    "\n",
    "# define the number of folds\n",
    "k = 10\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# initialize lists to store MSE values for each fold\n",
    "mse_list = []\n",
    "\n",
    "# perform K-fold cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # gradient Descent\n",
    "    for iteration in range(num_iterations):\n",
    "        # calculate predictions\n",
    "        predictions_train = np.dot(X_train, theta)\n",
    "\n",
    "        # calculate errors\n",
    "        errors = predictions_train - y_train\n",
    "\n",
    "        # update parameters using gradient with Elastic Net penalty\n",
    "        l1_penalty = (lambda_1 / len(y_train)) * np.sign(theta)\n",
    "        l2_penalty = (lambda_2 / len(y_train)) * theta\n",
    "        gradient = (1 / len(y_train)) * np.dot(X_train.T, errors) + l1_penalty + l2_penalty\n",
    "        theta -= alpha * gradient\n",
    "        \n",
    "        # calculate cost (Mean Squared Error)\n",
    "        cost = np.mean((np.dot(X_train, theta) - y_train) ** 2)\n",
    "\n",
    "    # evaluate on the test set for this fold\n",
    "    predictions_test = np.dot(X_test, theta)\n",
    "    mse_test = np.mean((predictions_test - y_test) ** 2)\n",
    "\n",
    "    mse_list.append(mse_test)\n",
    "\n",
    "# calculate and print the average MSE value across all folds\n",
    "average_mse = np.mean(mse_list)\n",
    "print(f'Average mean squared error across {k} folds: {average_mse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b8ddd5",
   "metadata": {},
   "source": [
    "### Chosen model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301fc26a",
   "metadata": {},
   "source": [
    "Multivariate linear regression. The parameters are the coefficients of each feature. It is simple and easy to work with, and it generally performs well when there are linear relationships between features and the target variable. In this case, multivariate linear regression with gradient descent gave better mse scores compared to Lasso and Elastic Net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c37f3eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
