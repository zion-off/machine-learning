{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "aiFCjWXUYQ_Y"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7dBQwQLGYWz0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the dataset\n",
    "digits = load_digits()\n",
    "\n",
    "# visualize the dataset (print some sample images)\n",
    "fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(8, 8))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap='gray')\n",
    "    ax.set_title(f\"Label: {digits.target[i]}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDzhGObpYbwE"
   },
   "source": [
    "# Using sigmoid function as activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZSF5q8FNOKKp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 1.2857095892906343\n",
      "Epoch 100: Loss 0.7913370141219627\n",
      "Epoch 200: Loss 0.6432962252727965\n",
      "Epoch 300: Loss 0.5740613339735923\n",
      "Epoch 400: Loss 0.5293623718162045\n",
      "Test Accuracy with LR=0.01 and Epochs=500: 0.23055555555555557, MSE: 13.33611111111111\n",
      "Epoch 0: Loss 1.935188366514842\n",
      "Epoch 100: Loss 0.9006823209107478\n",
      "Epoch 200: Loss 0.6341520238577586\n",
      "Epoch 300: Loss 0.5469380328503165\n",
      "Epoch 400: Loss 0.4991469699229747\n",
      "Epoch 500: Loss 0.46370233508117115\n",
      "Epoch 600: Loss 0.4337192109176069\n",
      "Epoch 700: Loss 0.4071921648164463\n",
      "Epoch 800: Loss 0.38346252640199685\n",
      "Epoch 900: Loss 0.362194857340903\n",
      "Test Accuracy with LR=0.01 and Epochs=1000: 0.49444444444444446, MSE: 9.722222222222221\n",
      "Epoch 0: Loss 2.0677792946854807\n",
      "Epoch 100: Loss 0.8575906957925863\n",
      "Epoch 200: Loss 0.6027693776381046\n",
      "Epoch 300: Loss 0.5156826920028043\n",
      "Epoch 400: Loss 0.4677387079373339\n",
      "Epoch 500: Loss 0.4334366331589582\n",
      "Epoch 600: Loss 0.4056884755575949\n",
      "Epoch 700: Loss 0.38199077719820085\n",
      "Epoch 800: Loss 0.36124539283375057\n",
      "Epoch 900: Loss 0.3428565071786258\n",
      "Epoch 1000: Loss 0.32643245665689674\n",
      "Epoch 1100: Loss 0.311681114523995\n",
      "Epoch 1200: Loss 0.2983703626198115\n",
      "Epoch 1300: Loss 0.28631392979214726\n",
      "Epoch 1400: Loss 0.27536161897767863\n",
      "Test Accuracy with LR=0.01 and Epochs=1500: 0.6722222222222223, MSE: 7.847222222222222\n",
      "Epoch 0: Loss 1.8634084298907767\n",
      "Epoch 100: Loss 0.49915472643192554\n",
      "Epoch 200: Loss 0.37261122052592865\n",
      "Epoch 300: Loss 0.2951689268523178\n",
      "Epoch 400: Loss 0.2456564536945491\n",
      "Test Accuracy with LR=0.05 and Epochs=500: 0.7444444444444445, MSE: 5.288888888888889\n",
      "Epoch 0: Loss 1.963132032250684\n",
      "Epoch 100: Loss 0.4663065459419743\n",
      "Epoch 200: Loss 0.35565812941867375\n",
      "Epoch 300: Loss 0.28654536697274635\n",
      "Epoch 400: Loss 0.24136144544745142\n",
      "Epoch 500: Loss 0.20972870949256733\n",
      "Epoch 600: Loss 0.18642474411501356\n",
      "Epoch 700: Loss 0.16859743962837503\n",
      "Epoch 800: Loss 0.1544803137111485\n",
      "Epoch 900: Loss 0.14294755295515207\n",
      "Test Accuracy with LR=0.05 and Epochs=1000: 0.8277777777777777, MSE: 3.0277777777777777\n",
      "Epoch 0: Loss 2.394987227069042\n",
      "Epoch 100: Loss 0.44878009238720634\n",
      "Epoch 200: Loss 0.3326648560418563\n",
      "Epoch 300: Loss 0.26641916551137695\n",
      "Epoch 400: Loss 0.22521743602949568\n",
      "Epoch 500: Loss 0.19733999296217827\n",
      "Epoch 600: Loss 0.17713244375499984\n",
      "Epoch 700: Loss 0.16165132541007518\n",
      "Epoch 800: Loss 0.14925178805525696\n",
      "Epoch 900: Loss 0.13901026923530532\n",
      "Epoch 1000: Loss 0.13041992493950808\n",
      "Epoch 1100: Loss 0.12306887402449584\n",
      "Epoch 1200: Loss 0.1166589696465271\n",
      "Epoch 1300: Loss 0.11098350627805063\n",
      "Epoch 1400: Loss 0.10588720454947961\n",
      "Test Accuracy with LR=0.05 and Epochs=1500: 0.9027777777777778, MSE: 1.788888888888889\n",
      "Epoch 0: Loss 2.2426064673118393\n",
      "Epoch 100: Loss 0.3500857587637696\n",
      "Epoch 200: Loss 0.22691692032635868\n",
      "Epoch 300: Loss 0.17286888919143953\n",
      "Epoch 400: Loss 0.14230552976941882\n",
      "Test Accuracy with LR=0.1 and Epochs=500: 0.8472222222222222, MSE: 3.3583333333333334\n",
      "Epoch 0: Loss 2.425597462661431\n",
      "Epoch 100: Loss 0.3372657690276844\n",
      "Epoch 200: Loss 0.22795318911495666\n",
      "Epoch 300: Loss 0.17932316470380383\n",
      "Epoch 400: Loss 0.15115212325795474\n",
      "Epoch 500: Loss 0.13206071261690447\n",
      "Epoch 600: Loss 0.1178626301240517\n",
      "Epoch 700: Loss 0.10660204719021088\n",
      "Epoch 800: Loss 0.09737165517460199\n",
      "Epoch 900: Loss 0.08971756655954244\n",
      "Test Accuracy with LR=0.1 and Epochs=1000: 0.9166666666666666, MSE: 2.2222222222222223\n",
      "Epoch 0: Loss 2.5403099951045416\n",
      "Epoch 100: Loss 0.34996843673829364\n",
      "Epoch 200: Loss 0.23728058147495482\n",
      "Epoch 300: Loss 0.18344502069757182\n",
      "Epoch 400: Loss 0.152098194726648\n",
      "Epoch 500: Loss 0.13140703074071045\n",
      "Epoch 600: Loss 0.11656623601707518\n",
      "Epoch 700: Loss 0.10515554031627199\n",
      "Epoch 800: Loss 0.09594437179146939\n",
      "Epoch 900: Loss 0.08840791042900419\n",
      "Epoch 1000: Loss 0.0820914652355001\n",
      "Epoch 1100: Loss 0.07669674014386164\n",
      "Epoch 1200: Loss 0.07199276771890574\n",
      "Epoch 1300: Loss 0.06784148914754137\n",
      "Epoch 1400: Loss 0.06413985640901138\n",
      "Test Accuracy with LR=0.1 and Epochs=1500: 0.9305555555555556, MSE: 1.1694444444444445\n"
     ]
    }
   ],
   "source": [
    "# extract the features and labels\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# creating training and test dataset split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# scaling the dataset features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # initialize weights and biases\n",
    "        self.W1 = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.b1 = np.zeros((1, self.hidden_size))\n",
    "        self.W2 = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.b2 = np.zeros((1, self.output_size))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # forward propagation\n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.probs = self.sigmoid(self.z2)\n",
    "        return self.probs\n",
    "\n",
    "    # backpropagation\n",
    "    def backward(self, X, y, learning_rate=0.01):\n",
    "        m = X.shape[0]\n",
    "\n",
    "        # calculate gradients\n",
    "        dZ2 = self.probs - y\n",
    "        dW2 = (1 / m) * np.dot(self.a1.T, dZ2)\n",
    "        db2 = (1 / m) * np.sum(dZ2, axis=0, keepdims=True)\n",
    "        dZ1 = np.dot(dZ2, self.W2.T) * (self.a1 * (1 - self.a1))\n",
    "        dW1 = (1 / m) * np.dot(X.T, dZ1)\n",
    "        db1 = (1 / m) * np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "        # update weights and biases\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "\n",
    "    def train(self, X, y, epochs=1000, learning_rate=0.01):\n",
    "        for epoch in range(epochs):\n",
    "            # forward and backward propagation\n",
    "            self.forward(X)\n",
    "            self.backward(X, y, learning_rate)\n",
    "\n",
    "            # calculate and print the loss\n",
    "            if epoch % 100 == 0:\n",
    "                loss = self.calculate_loss(X, y)\n",
    "                print(f'Epoch {epoch}: Loss {loss}')\n",
    "\n",
    "    def calculate_loss(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        loss = np.sum((self.probs - y) ** 2) / (2 * m)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "# applying one hot encoding to targets\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    num_samples = len(labels)\n",
    "    one_hot_labels = np.zeros((num_samples, num_classes))\n",
    "    for i in range(num_samples):\n",
    "        one_hot_labels[i, labels[i]] = 1\n",
    "    return one_hot_labels\n",
    "\n",
    "def train_and_evaluate_model(learning_rate, epochs):\n",
    "    num_classes = 10\n",
    "    y_train_encoded = one_hot_encode(y_train, num_classes).astype(int)\n",
    "    y_test_encoded = one_hot_encode(y_test, num_classes).astype(int)\n",
    "\n",
    "    # initialize and train the neural network\n",
    "    input_size = X_train.shape[1]\n",
    "    hidden_size = 30\n",
    "    output_size = num_classes\n",
    "\n",
    "    model = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "    model.train(X_train, y_train_encoded, epochs=epochs, learning_rate=learning_rate)\n",
    "\n",
    "    # evaluate the model on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # calculate accuracy\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "\n",
    "    # calculate mse\n",
    "    mse = np.mean((y_pred - y_test) ** 2)\n",
    "\n",
    "    return accuracy, mse\n",
    "\n",
    "# test with different learning rates and epochs\n",
    "learning_rates = [0.01, 0.05, 0.1]\n",
    "epochs_list = [500, 1000, 1500]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for epochs in epochs_list:\n",
    "        acc, mse = train_and_evaluate_model(lr, epochs)\n",
    "        print(f'Test Accuracy with LR={lr} and Epochs={epochs}: {acc}, MSE: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0ElHj54Y0tN"
   },
   "source": [
    "# Using ReLU function as activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mlnTMKyrHfG3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 2377.9911782234926\n",
      "Epoch 100: Loss 2.420101556958201\n",
      "Epoch 200: Loss 1.8821989566119657\n",
      "Epoch 300: Loss 1.544630780853313\n",
      "Epoch 400: Loss 1.3124264934037495\n",
      "Test Accuracy with LR=0.01 and Epochs=500: 0.3333333333333333, MSE: 20.76388888888889\n",
      "Epoch 0: Loss 2180.907646605701\n",
      "Epoch 100: Loss 2.4375699043816765\n",
      "Epoch 200: Loss 1.8860151234896407\n",
      "Epoch 300: Loss 1.504704825937911\n",
      "Epoch 400: Loss 1.2677802540282184\n",
      "Epoch 500: Loss 1.130207654209413\n",
      "Epoch 600: Loss 1.0300297313184652\n",
      "Epoch 700: Loss 0.9390439163981595\n",
      "Epoch 800: Loss 0.8624966803386042\n",
      "Epoch 900: Loss 0.7950053937809276\n",
      "Test Accuracy with LR=0.01 and Epochs=1000: 0.49444444444444446, MSE: 14.644444444444444\n",
      "Epoch 0: Loss 1436.8412297971747\n",
      "Epoch 100: Loss 2.482283043613284\n",
      "Epoch 200: Loss 1.837319704893483\n",
      "Epoch 300: Loss 1.439029283385576\n",
      "Epoch 400: Loss 1.2006916557319707\n",
      "Epoch 500: Loss 1.0458474436564935\n",
      "Epoch 600: Loss 0.9311327217123303\n",
      "Epoch 700: Loss 0.8389130240069465\n",
      "Epoch 800: Loss 0.761174781525022\n",
      "Epoch 900: Loss 0.7013553971829755\n",
      "Epoch 1000: Loss 0.65075669741398\n",
      "Epoch 1100: Loss 0.6057970658534131\n",
      "Epoch 1200: Loss 0.5668612247884981\n",
      "Epoch 1300: Loss 0.5294765046878072\n",
      "Epoch 1400: Loss 0.49667012758710116\n",
      "Test Accuracy with LR=0.01 and Epochs=1500: 0.5861111111111111, MSE: 9.833333333333334\n",
      "Epoch 0: Loss 1311.600386508153\n",
      "Epoch 100: Loss 1.1980191934090485\n",
      "Epoch 200: Loss 0.7469778109444586\n",
      "Epoch 300: Loss 0.5386489815803623\n",
      "Epoch 400: Loss 0.4117118687789313\n",
      "Test Accuracy with LR=0.05 and Epochs=500: 0.6055555555555555, MSE: 11.683333333333334\n",
      "Epoch 0: Loss 2773.1640910066862\n",
      "Epoch 100: Loss 0.8547245159259163\n",
      "Epoch 200: Loss 0.5539111796326095\n",
      "Epoch 300: Loss 0.4467498287779457\n",
      "Epoch 400: Loss 0.3789315362194051\n",
      "Epoch 500: Loss 0.33900673907137807\n",
      "Epoch 600: Loss 0.3154677005203386\n",
      "Epoch 700: Loss 0.2969661188232432\n",
      "Epoch 800: Loss 0.28294447434531134\n",
      "Epoch 900: Loss 0.2719509772276673\n",
      "Test Accuracy with LR=0.05 and Epochs=1000: 0.5166666666666667, MSE: 16.01388888888889\n",
      "Epoch 0: Loss 2776.7890622854793\n",
      "Epoch 100: Loss 0.8163947049358554\n",
      "Epoch 200: Loss 0.5504006567505387\n",
      "Epoch 300: Loss 0.4403709436511269\n",
      "Epoch 400: Loss 0.3846429720539742\n",
      "Epoch 500: Loss 0.34558921072736826\n",
      "Epoch 600: Loss 0.3196925249508203\n",
      "Epoch 700: Loss 0.29941312409423815\n",
      "Epoch 800: Loss 0.28304968086844634\n",
      "Epoch 900: Loss 0.2692553510653008\n",
      "Epoch 1000: Loss 0.2563331197971112\n",
      "Epoch 1100: Loss 0.24488711176613923\n",
      "Epoch 1200: Loss 0.23572679977814423\n",
      "Epoch 1300: Loss 0.2286312744444642\n",
      "Epoch 1400: Loss 0.22266552024265587\n",
      "Test Accuracy with LR=0.05 and Epochs=1500: 0.6138888888888889, MSE: 10.38888888888889\n",
      "Epoch 0: Loss 1573.8625621838403\n",
      "Epoch 100: Loss 0.5098974408111371\n",
      "Epoch 200: Loss 0.36569166431752703\n",
      "Epoch 300: Loss 0.30855104031011304\n",
      "Epoch 400: Loss 0.2799028341345719\n",
      "Test Accuracy with LR=0.1 and Epochs=500: 0.4861111111111111, MSE: 10.466666666666667\n",
      "Epoch 0: Loss 2422.8100087643215\n",
      "Epoch 100: Loss 1.2005827917119982\n",
      "Epoch 200: Loss 0.6666020437623552\n",
      "Epoch 300: Loss 0.6123220992142505\n",
      "Epoch 400: Loss 0.4144016588017091\n",
      "Epoch 500: Loss 0.29596330213301497\n",
      "Epoch 600: Loss 0.32349219850505123\n",
      "Epoch 700: Loss 0.24777813649780608\n",
      "Epoch 800: Loss 0.2804122240139254\n",
      "Epoch 900: Loss 0.2332882519028861\n",
      "Test Accuracy with LR=0.1 and Epochs=1000: 0.6666666666666666, MSE: 9.813888888888888\n",
      "Epoch 0: Loss 2256.1166354373745\n",
      "Epoch 100: Loss 0.5574747469008257\n",
      "Epoch 200: Loss 0.480809681208718\n",
      "Epoch 300: Loss 0.31548015991328193\n",
      "Epoch 400: Loss 0.294411402914838\n",
      "Epoch 500: Loss 0.261747005172985\n",
      "Epoch 600: Loss 0.24278462266016942\n",
      "Epoch 700: Loss 0.22712569545610176\n",
      "Epoch 800: Loss 0.21405384052226625\n",
      "Epoch 900: Loss 0.19817231064743007\n",
      "Epoch 1000: Loss 0.183740309338957\n",
      "Epoch 1100: Loss 0.1724530838962701\n",
      "Epoch 1200: Loss 0.16323490983178326\n",
      "Epoch 1300: Loss 0.15656551232056745\n",
      "Epoch 1400: Loss 0.15172304796997255\n",
      "Test Accuracy with LR=0.1 and Epochs=1500: 0.7194444444444444, MSE: 5.361111111111111\n"
     ]
    }
   ],
   "source": [
    "# extract the features and labels\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# creating training and test dataset split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# scaling the dataset features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # initialize weights and biases\n",
    "        self.W1 = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.b1 = np.zeros((1, self.hidden_size))\n",
    "        self.W2 = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.b2 = np.zeros((1, self.output_size))\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    # forward propagation\n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.output = self.relu(self.z2)\n",
    "        return self.output\n",
    "\n",
    "    # backpropagation pass\n",
    "    def backward(self, X, y, learning_rate=0.01):\n",
    "        m = X.shape[0]\n",
    "\n",
    "        # calculate gradients\n",
    "        dZ2 = self.output - y\n",
    "        dW2 = (1 / m) * np.dot(self.a1.T, dZ2)\n",
    "        db2 = (1 / m) * np.sum(dZ2, axis=0, keepdims=True)\n",
    "        dZ1 = np.dot(dZ2, self.W2.T) * (self.a1 > 0)\n",
    "        dW1 = (1 / m) * np.dot(X.T, dZ1)\n",
    "        db1 = (1 / m) * np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "        # update weights and biases\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "\n",
    "    def train(self, X, y, epochs=1000, learning_rate=0.01):\n",
    "        for epoch in range(epochs):\n",
    "            # forward and backward propagation\n",
    "            self.forward(X)\n",
    "            self.backward(X, y, learning_rate)\n",
    "\n",
    "            # calculate and print the loss\n",
    "            if epoch % 100 == 0:\n",
    "                loss = self.calculate_loss(X, y)\n",
    "                print(f'Epoch {epoch}: Loss {loss}')\n",
    "\n",
    "    def calculate_loss(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        loss = np.sum((self.output - y) ** 2) / (2 * m)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, X):\n",
    "        output = self.forward(X)\n",
    "        return np.argmax(output, axis=1)\n",
    "\n",
    "# applying one hot encoding to targets\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    num_samples = len(labels)\n",
    "    one_hot_labels = np.zeros((num_samples, num_classes))\n",
    "    for i in range(num_samples):\n",
    "        one_hot_labels[i, labels[i]] = 1\n",
    "    return one_hot_labels\n",
    "\n",
    "def train_and_evaluate_model(learning_rate, epochs):\n",
    "    num_classes = 10\n",
    "    y_train_encoded = one_hot_encode(y_train, num_classes).astype(int)\n",
    "    y_test_encoded = one_hot_encode(y_test, num_classes).astype(int)\n",
    "\n",
    "    # initialize and train the neural network\n",
    "    input_size = X_train.shape[1]\n",
    "    hidden_size = 30\n",
    "    output_size = num_classes\n",
    "\n",
    "    model = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "    model.train(X_train, y_train_encoded, epochs=epochs, learning_rate=learning_rate)\n",
    "\n",
    "    # evaluate the model on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # calculate accuracy\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "\n",
    "    # calculate mse\n",
    "    mse = np.mean((y_pred - y_test) ** 2)\n",
    "\n",
    "    return accuracy, mse\n",
    "\n",
    "# test with different learning rates and epochs\n",
    "learning_rates = [0.01, 0.05, 0.1]\n",
    "epochs_list = [500, 1000, 1500]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for epochs in epochs_list:\n",
    "        acc, mse = train_and_evaluate_model(lr, epochs)\n",
    "        print(f'Test Accuracy with LR={lr} and Epochs={epochs}: {acc}, MSE: {mse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ag-C4zZtZQvj"
   },
   "source": [
    "# Using tanh function as activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "w_dULMOBKk6R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 131.50180314066372\n",
      "Epoch 100: Loss 17.474920358116748\n",
      "Epoch 200: Loss 8.039602718986353\n",
      "Epoch 300: Loss 4.515868651074103\n",
      "Epoch 400: Loss 2.778017109849583\n",
      "Test Accuracy with LR=0.01 and Epochs=500: 0.29444444444444445, MSE: 11.23611111111111\n",
      "Epoch 0: Loss 115.61274482626264\n",
      "Epoch 100: Loss 18.32631099001602\n",
      "Epoch 200: Loss 8.806871701006731\n",
      "Epoch 300: Loss 5.017100254615805\n",
      "Epoch 400: Loss 3.0909688392112216\n",
      "Epoch 500: Loss 2.012024808772038\n",
      "Epoch 600: Loss 1.3730848139690293\n",
      "Epoch 700: Loss 0.9806135159115883\n",
      "Epoch 800: Loss 0.7331559616560207\n",
      "Epoch 900: Loss 0.573973975229727\n",
      "Test Accuracy with LR=0.01 and Epochs=1000: 0.6055555555555555, MSE: 8.716666666666667\n",
      "Epoch 0: Loss 119.10445486562081\n",
      "Epoch 100: Loss 17.644296891097135\n",
      "Epoch 200: Loss 8.29326627548494\n",
      "Epoch 300: Loss 4.658313325309511\n",
      "Epoch 400: Loss 2.8432156703282527\n",
      "Epoch 500: Loss 1.8408476169130927\n",
      "Epoch 600: Loss 1.2537848157162728\n",
      "Epoch 700: Loss 0.8965643274805761\n",
      "Epoch 800: Loss 0.6731796137829035\n",
      "Epoch 900: Loss 0.5305874509411354\n",
      "Epoch 1000: Loss 0.4380981621473635\n",
      "Epoch 1100: Loss 0.37733267533675124\n",
      "Epoch 1200: Loss 0.33698623912491144\n",
      "Epoch 1300: Loss 0.3099564341158469\n",
      "Epoch 1400: Loss 0.29170433302765214\n",
      "Test Accuracy with LR=0.01 and Epochs=1500: 0.7444444444444445, MSE: 4.883333333333334\n",
      "Epoch 0: Loss 146.37985814160206\n",
      "Epoch 100: Loss 2.2985402432048385\n",
      "Epoch 200: Loss 0.5397618825407241\n",
      "Epoch 300: Loss 0.2974633447730114\n",
      "Epoch 400: Loss 0.2551336626881919\n",
      "Test Accuracy with LR=0.05 and Epochs=500: 0.7805555555555556, MSE: 3.4972222222222222\n",
      "Epoch 0: Loss 126.03943648376527\n",
      "Epoch 100: Loss 2.253000265913059\n",
      "Epoch 200: Loss 0.5417431923759409\n",
      "Epoch 300: Loss 0.30846407624531597\n",
      "Epoch 400: Loss 0.26600440864304764\n",
      "Epoch 500: Loss 0.25648711719447087\n",
      "Epoch 600: Loss 0.25341749948428777\n",
      "Epoch 700: Loss 0.25169129670958956\n",
      "Epoch 800: Loss 0.2502639259845132\n",
      "Epoch 900: Loss 0.24891906177457934\n",
      "Test Accuracy with LR=0.05 and Epochs=1000: 0.825, MSE: 3.85\n",
      "Epoch 0: Loss 147.12228550315615\n",
      "Epoch 100: Loss 2.1881382347824756\n",
      "Epoch 200: Loss 0.50441864772499\n",
      "Epoch 300: Loss 0.29702242403568146\n",
      "Epoch 400: Loss 0.2632458202257125\n",
      "Epoch 500: Loss 0.25627964722730806\n",
      "Epoch 600: Loss 0.2539597335679593\n",
      "Epoch 700: Loss 0.25248946148258994\n",
      "Epoch 800: Loss 0.2511766301296866\n",
      "Epoch 900: Loss 0.24989144043691128\n",
      "Epoch 1000: Loss 0.24861148541796124\n",
      "Epoch 1100: Loss 0.24733751115168232\n",
      "Epoch 1200: Loss 0.24607665805128937\n",
      "Epoch 1300: Loss 0.24483720577227489\n",
      "Epoch 1400: Loss 0.24362533042856352\n",
      "Test Accuracy with LR=0.05 and Epochs=1500: 0.7944444444444444, MSE: 3.8222222222222224\n",
      "Epoch 0: Loss 135.64426188954718\n",
      "Epoch 100: Loss 0.49975664962401645\n",
      "Epoch 200: Loss 0.2525034968392467\n",
      "Epoch 300: Loss 0.2401860901741505\n",
      "Epoch 400: Loss 0.23728705571764552\n",
      "Test Accuracy with LR=0.1 and Epochs=500: 0.8194444444444444, MSE: 3.341666666666667\n",
      "Epoch 0: Loss 132.48311192744794\n",
      "Epoch 100: Loss 0.4850117705144774\n",
      "Epoch 200: Loss 0.26520367990862115\n",
      "Epoch 300: Loss 0.25560474349184586\n",
      "Epoch 400: Loss 0.25270928833592604\n",
      "Epoch 500: Loss 0.2501487370619485\n",
      "Epoch 600: Loss 0.24770245716422948\n",
      "Epoch 700: Loss 0.24536755497455376\n",
      "Epoch 800: Loss 0.24313950324837924\n",
      "Epoch 900: Loss 0.24101657843093643\n",
      "Test Accuracy with LR=0.1 and Epochs=1000: 0.7694444444444445, MSE: 4.463888888888889\n",
      "Epoch 0: Loss 133.6131790249004\n",
      "Epoch 100: Loss 0.4535500588008926\n",
      "Epoch 200: Loss 0.2453181376174524\n",
      "Epoch 300: Loss 0.2358955819725657\n",
      "Epoch 400: Loss 0.2334565208670365\n",
      "Epoch 500: Loss 0.2314134882403478\n",
      "Epoch 600: Loss 0.22944342700434361\n",
      "Epoch 700: Loss 0.22754469184238077\n",
      "Epoch 800: Loss 0.22574959903099445\n",
      "Epoch 900: Loss 0.2240596047925979\n",
      "Epoch 1000: Loss 0.22246113764638287\n",
      "Epoch 1100: Loss 0.22094139886731662\n",
      "Epoch 1200: Loss 0.21948960096878264\n",
      "Epoch 1300: Loss 0.21809602096673064\n",
      "Epoch 1400: Loss 0.2167512832377\n",
      "Test Accuracy with LR=0.1 and Epochs=1500: 0.8583333333333333, MSE: 3.3222222222222224\n"
     ]
    }
   ],
   "source": [
    "# extract the features and labels\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# creating training and test dataset split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# scaling the dataset features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # initialize weights and biases\n",
    "        self.W1 = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.b1 = np.zeros((1, self.hidden_size))\n",
    "        self.W2 = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.b2 = np.zeros((1, self.output_size))\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    # forward propagation\n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.tanh(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        return self.z2\n",
    "\n",
    "    # backpropagation pass\n",
    "    def backward(self, X, y, learning_rate=0.01):\n",
    "        m = X.shape[0]\n",
    "\n",
    "        dZ2 = self.z2 - y\n",
    "        dW2 = (1 / m) * np.dot(self.a1.T, dZ2)\n",
    "        db2 = (1 / m) * np.sum(dZ2, axis=0, keepdims=True)\n",
    "        dZ1 = np.dot(dZ2, self.W2.T) * (1 - np.power(self.a1, 2))\n",
    "        dW1 = (1 / m) * np.dot(X.T, dZ1)\n",
    "        db1 = (1 / m) * np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "        # update weights and biases\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "\n",
    "    def train(self, X, y, epochs=1000, learning_rate=0.01):\n",
    "        for epoch in range(epochs):\n",
    "            # forward propagation\n",
    "            self.forward(X)\n",
    "\n",
    "            # backpropagation\n",
    "            self.backward(X, y, learning_rate)\n",
    "\n",
    "            # calculate and print the loss\n",
    "            if epoch % 100 == 0:\n",
    "                loss = self.calculate_loss(X, y)\n",
    "                print(f'Epoch {epoch}: Loss {loss}')\n",
    "\n",
    "    def calculate_loss(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        loss = np.sum(np.square(self.z2 - y)) / (2 * m)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, X):\n",
    "        logits = self.forward(X)\n",
    "        return np.argmax(logits, axis=1)\n",
    "\n",
    "# applying one hot encoding to targets\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    num_samples = len(labels)\n",
    "    one_hot_labels = np.zeros((num_samples, num_classes))\n",
    "    for i in range(num_samples):\n",
    "        one_hot_labels[i, labels[i]] = 1\n",
    "    return one_hot_labels\n",
    "\n",
    "def test_model(learning_rate, epochs):\n",
    "    num_classes = 10\n",
    "    y_train_encoded = one_hot_encode(y_train, num_classes).astype(int)\n",
    "    y_test_encoded = one_hot_encode(y_test, num_classes).astype(int)\n",
    "\n",
    "    # initialize and train the neural network\n",
    "    input_size = X_train.shape[1]\n",
    "    hidden_size = 30\n",
    "    output_size = num_classes\n",
    "\n",
    "    model = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "    model.train(X_train, y_train_encoded, epochs=epochs, learning_rate=learning_rate)\n",
    "\n",
    "    # evaluate the model on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # calculate accuracy\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "\n",
    "    # calculate mse\n",
    "    mse = np.mean((y_pred - y_test) ** 2)\n",
    "\n",
    "    return accuracy, mse\n",
    "\n",
    "# test with different learning rates and epochs\n",
    "learning_rates = [0.01, 0.05, 0.1]\n",
    "epochs_list = [500, 1000, 1500]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for epochs in epochs_list:\n",
    "        acc, mse = test_model(lr, epochs)\n",
    "        print(f'Test Accuracy with LR={lr} and Epochs={epochs}: {acc}, MSE: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
